{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"naive_bayes_G.231.19.0142.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNr+QC5jTYtVK7ehqKPRpxY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"-T6Oum7eyAcx","executionInfo":{"status":"ok","timestamp":1637286564748,"user_tz":-420,"elapsed":833,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["#load libraries\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn import datasets\n","#import train_test_split function\n","from sklearn.model_selection import train_test_split\n","#import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"SusKOowXyRdZ","executionInfo":{"status":"ok","timestamp":1637286564751,"user_tz":-420,"elapsed":22,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["#load data\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"cM_LdiXiyXOS","executionInfo":{"status":"ok","timestamp":1637286564753,"user_tz":-420,"elapsed":20,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["#split dataset into training set and test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXXfHOyPyb7Y","executionInfo":{"status":"ok","timestamp":1637286565255,"user_tz":-420,"elapsed":520,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["#create adaboost classifier object\n","abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)\n","#training adaboost classifier\n","model = abc.fit(X_train, y_train)\n","#predict the response for test dataset\n","y_pred = model.predict(X_test)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSIPcrWSyeHX","executionInfo":{"status":"ok","timestamp":1637286565256,"user_tz":-420,"elapsed":17,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}},"outputId":"bdf374ea-90a6-4c03-debc-35c10d222330"},"source":["#model accuracy, how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9555555555555556\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3xzzHzgUylKy","executionInfo":{"status":"ok","timestamp":1637286566400,"user_tz":-420,"elapsed":1155,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}},"outputId":"6be72f32-8884-4c65-ad1f-1dccbe46bf2d"},"source":["!git clone https://github.com/eriklindernoren/ML-From-Scratch"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ML-From-Scratch'...\n","remote: Enumerating objects: 2558, done.\u001b[K\n","remote: Total 2558 (delta 0), reused 0 (delta 0), pack-reused 2558\u001b[K\n","Receiving objects: 100% (2558/2558), 553.45 KiB | 5.22 MiB/s, done.\n","Resolving deltas: 100% (1960/1960), done.\n"]}]},{"cell_type":"code","metadata":{"id":"muX3Z9ZSylAv","executionInfo":{"status":"ok","timestamp":1637286566402,"user_tz":-420,"elapsed":19,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["import sys\n","sys.path.append('/content/ML-From-Scratch')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"TUp3ZVGuzND3","executionInfo":{"status":"ok","timestamp":1637286639500,"user_tz":-420,"elapsed":389,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["from __future__ import division, print_function\n","import numpy as np\n","import math\n","from mlfromscratch.utils import train_test_split, normalize\n","from mlfromscratch.utils import Plot, accuracy_score\n","\n","class NaiveBayes():\n","    \"\"\"The Gaussian Naive Bayes classifier. \"\"\"\n","    def fit(self, X, y):\n","        self.X, self.y = X, y\n","        self.classes = np.unique(y)\n","        self.parameters = []\n","        # Calculate the mean and variance of each feature for each class\n","        for i, c in enumerate(self.classes):\n","            # Only select the rows where the label equals the given class\n","            X_where_c = X[np.where(y == c)]\n","            self.parameters.append([])\n","            # Add the mean and variance for each feature (column)\n","            for col in X_where_c.T:\n","                parameters = {\"mean\": col.mean(), \"var\": col.var()}\n","                self.parameters[i].append(parameters)\n","\n","    def _calculate_likelihood(self, mean, var, x):\n","        \"\"\" Gaussian likelihood of the data x given mean and var \"\"\"\n","        eps = 1e-4 # Added in denominator to prevent division by zero\n","        coeff = 1.0 / math.sqrt(2.0 * math.pi * var + eps)\n","        exponent = math.exp(-(math.pow(x - mean, 2) / (2 * var + eps)))\n","        return coeff * exponent\n","\n","    def _calculate_prior(self, c):\n","        \"\"\" Calculate the prior of class c\n","        (samples where class == c / total number of samples)\"\"\"\n","        frequency = np.mean(self.y == c)\n","        return frequency\n","\n","    def _classify(self, sample):\n","        \"\"\" Classification using Bayes Rule P(Y|X) = P(X|Y)*P(Y)/P(X),\n","            or Posterior = Likelihood * Prior / Scaling Factor\n","\n","        P(Y|X) - The posterior is the probability that sample x is of class y given the\n","                 feature values of x being distributed according to distribution of y and the prior.\n","        P(X|Y) - Likelihood of data X given class distribution Y.\n","                 Gaussian distribution (given by _calculate_likelihood)\n","        P(Y)   - Prior (given by _calculate_prior)\n","        P(X)   - Scales the posterior to make it a proper probability distribution.\n","                 This term is ignored in this implementation since it doesn't affect\n","                 which class distribution the sample is most likely to belong to.\n","\n","        Classifies the sample as the class that results in the largest P(Y|X) (posterior)\n","        \"\"\"\n","        posteriors = []\n","        # Go through list of classes\n","        for i, c in enumerate(self.classes):\n","            # Initialize posterior as prior\n","            posterior = self._calculate_prior(c)\n","            # Naive assumption (independence):\n","            # P(x1,x2,x3|Y) = P(x1|Y)*P(x2|Y)*P(x3|Y)\n","            # Posterior is product of prior and likelihoods (ignoring scaling factor)\n","            for feature_value, params in zip(sample, self.parameters[i]):\n","                # Likelihood of feature value given distribution of feature values given y\n","                likelihood = self._calculate_likelihood(params[\"mean\"], params[\"var\"], feature_value)\n","                posterior *= likelihood\n","            posteriors.append(posterior)\n","        # Return the class with the largest posterior probability\n","        return self.classes[np.argmax(posteriors)]\n","\n","    def predict(self, X):\n","        \"\"\" Predict the class labels of the samples in X \"\"\"\n","        y_pred = [self._classify(sample) for sample in X]\n","        return y_pred\n"],"execution_count":9,"outputs":[]}]}