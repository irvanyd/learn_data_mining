{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bayesian_regression_G.231.19.0142.ipynb","provenance":[],"authorship_tag":"ABX9TyMxPQC80THDEKvz7H3pRftg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"-T6Oum7eyAcx","executionInfo":{"status":"ok","timestamp":1637284886128,"user_tz":-420,"elapsed":1119,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["#load libraries\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn import datasets\n","#import train_test_split function\n","from sklearn.model_selection import train_test_split\n","#import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"SusKOowXyRdZ","executionInfo":{"status":"ok","timestamp":1637284918690,"user_tz":-420,"elapsed":542,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["#load data\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"cM_LdiXiyXOS","executionInfo":{"status":"ok","timestamp":1637284939310,"user_tz":-420,"elapsed":402,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["#split dataset into training set and test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXXfHOyPyb7Y","executionInfo":{"status":"ok","timestamp":1637284957908,"user_tz":-420,"elapsed":435,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["#create adaboost classifier object\n","abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)\n","#training adaboost classifier\n","model = abc.fit(X_train, y_train)\n","#predict the response for test dataset\n","y_pred = model.predict(X_test)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSIPcrWSyeHX","executionInfo":{"status":"ok","timestamp":1637284979350,"user_tz":-420,"elapsed":444,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}},"outputId":"bea920e6-68a5-424c-a28b-0aedcf093120"},"source":["#model accuracy, how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8888888888888888\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3xzzHzgUylKy","executionInfo":{"status":"ok","timestamp":1637285014934,"user_tz":-420,"elapsed":1541,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}},"outputId":"3a64c7aa-0aef-4b5e-d777-ea632c0380dc"},"source":["!git clone https://github.com/eriklindernoren/ML-From-Scratch"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ML-From-Scratch'...\n","remote: Enumerating objects: 2558, done.\u001b[K\n","remote: Total 2558 (delta 0), reused 0 (delta 0), pack-reused 2558\u001b[K\n","Receiving objects: 100% (2558/2558), 553.45 KiB | 14.56 MiB/s, done.\n","Resolving deltas: 100% (1960/1960), done.\n"]}]},{"cell_type":"code","metadata":{"id":"muX3Z9ZSylAv","executionInfo":{"status":"ok","timestamp":1637285039863,"user_tz":-420,"elapsed":401,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["import sys\n","sys.path.append('/content/ML-From-Scratch')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"TUp3ZVGuzND3","executionInfo":{"status":"ok","timestamp":1637285160907,"user_tz":-420,"elapsed":678,"user":{"displayName":"irvan yudha andika","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUIlzrJ9OXIaoOtpuog6eTKFdxzzVIAqtptr1FzQ=s64","userId":"15130068374451695055"}}},"source":["from __future__ import print_function, division\n","import numpy as np\n","from scipy.stats import chi2, multivariate_normal\n","from mlfromscratch.utils import mean_squared_error, train_test_split, polynomial_features\n","\n","\n","\n","class BayesianRegression(object):\n","    \"\"\"Bayesian regression model. If poly_degree is specified the features will\n","    be transformed to with a polynomial basis function, which allows for polynomial\n","    regression. Assumes Normal prior and likelihood for the weights and scaled inverse\n","    chi-squared prior and likelihood for the variance of the weights.\n","\n","    Parameters:\n","    -----------\n","    n_draws: float\n","        The number of simulated draws from the posterior of the parameters.\n","    mu0: array\n","        The mean values of the prior Normal distribution of the parameters.\n","    omega0: array\n","        The precision matrix of the prior Normal distribution of the parameters.\n","    nu0: float\n","        The degrees of freedom of the prior scaled inverse chi squared distribution.\n","    sigma_sq0: float\n","        The scale parameter of the prior scaled inverse chi squared distribution.\n","    poly_degree: int\n","        The polynomial degree that the features should be transformed to. Allows\n","        for polynomial regression.\n","    cred_int: float\n","        The credible interval (ETI in this impl.). 95 => 95% credible interval of the posterior\n","        of the parameters.\n","\n","    Reference:\n","        https://github.com/mattiasvillani/BayesLearnCourse/raw/master/Slides/BayesLearnL5.pdf\n","    \"\"\"\n","    def __init__(self, n_draws, mu0, omega0, nu0, sigma_sq0, poly_degree=0, cred_int=95):\n","        self.w = None\n","        self.n_draws = n_draws\n","        self.poly_degree = poly_degree\n","        self.cred_int = cred_int\n","\n","        # Prior parameters\n","        self.mu0 = mu0\n","        self.omega0 = omega0\n","        self.nu0 = nu0\n","        self.sigma_sq0 = sigma_sq0\n","\n","    # Allows for simulation from the scaled inverse chi squared\n","    # distribution. Assumes the variance is distributed according to\n","    # this distribution.\n","    # Reference:\n","    #   https://en.wikipedia.org/wiki/Scaled_inverse_chi-squared_distribution\n","    def _draw_scaled_inv_chi_sq(self, n, df, scale):\n","        X = chi2.rvs(size=n, df=df)\n","        sigma_sq = df * scale / X\n","        return sigma_sq\n","\n","    def fit(self, X, y):\n","\n","        # If polynomial transformation\n","        if self.poly_degree:\n","            X = polynomial_features(X, degree=self.poly_degree)\n","\n","        n_samples, n_features = np.shape(X)\n","\n","        X_X = X.T.dot(X)\n","\n","        # Least squares approximate of beta\n","        beta_hat = np.linalg.pinv(X_X).dot(X.T).dot(y)\n","\n","        # The posterior parameters can be determined analytically since we assume\n","        # conjugate priors for the likelihoods.\n","\n","        # Normal prior / likelihood => Normal posterior\n","        mu_n = np.linalg.pinv(X_X + self.omega0).dot(X_X.dot(beta_hat)+self.omega0.dot(self.mu0))\n","        omega_n = X_X + self.omega0\n","        # Scaled inverse chi-squared prior / likelihood => Scaled inverse chi-squared posterior\n","        nu_n = self.nu0 + n_samples\n","        sigma_sq_n = (1.0/nu_n)*(self.nu0*self.sigma_sq0 + \\\n","            (y.T.dot(y) + self.mu0.T.dot(self.omega0).dot(self.mu0) - mu_n.T.dot(omega_n.dot(mu_n))))\n","\n","        # Simulate parameter values for n_draws\n","        beta_draws = np.empty((self.n_draws, n_features))\n","        for i in range(self.n_draws):\n","            sigma_sq = self._draw_scaled_inv_chi_sq(n=1, df=nu_n, scale=sigma_sq_n)\n","            beta = multivariate_normal.rvs(size=1, mean=mu_n[:,0], cov=sigma_sq*np.linalg.pinv(omega_n))\n","            # Save parameter draws\n","            beta_draws[i, :] = beta\n","\n","        # Select the mean of the simulated variables as the ones used to make predictions\n","        self.w = np.mean(beta_draws, axis=0)\n","\n","        # Lower and upper boundary of the credible interval\n","        l_eti = 50 - self.cred_int/2\n","        u_eti = 50 + self.cred_int/2\n","        self.eti = np.array([[np.percentile(beta_draws[:,i], q=l_eti), np.percentile(beta_draws[:,i], q=u_eti)] \\\n","                                for i in range(n_features)])\n","\n","    def predict(self, X, eti=False):\n","\n","        # If polynomial transformation\n","        if self.poly_degree:\n","            X = polynomial_features(X, degree=self.poly_degree)\n","\n","        y_pred = X.dot(self.w)\n","        # If the lower and upper boundaries for the 95%\n","        # equal tail interval should be returned\n","        if eti:\n","            lower_w = self.eti[:, 0]\n","            upper_w = self.eti[:, 1]\n","            y_lower_pred = X.dot(lower_w)\n","            y_upper_pred = X.dot(upper_w)\n","            return y_pred, y_lower_pred, y_upper_pred\n","\n","        return y_pred\n"],"execution_count":9,"outputs":[]}]}